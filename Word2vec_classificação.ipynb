{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2vec_classificação.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-VyvuGhkVal",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdb64e53-afdc-4c91-a7ca-21ed29b9efa6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcY3oQpRkp75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ceb7d1f-27b9-470f-a4fb-723584e4ff13"
      },
      "source": [
        "!unzip \"/content/drive/MyDrive/PLN/criticas-imdb.zip\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/PLN/criticas-imdb.zip\n",
            "  inflating: criticas-imdb.csv       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ1wyBJ4k7BV"
      },
      "source": [
        "#importando o conjunto de dados\n",
        "import pandas as pd\n",
        "\n",
        "criticas  = pd.read_csv(\"criticas-imdb.csv\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CK3sKmZRk-vc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "260599b7-415e-4a03-f889-33cbae2cc8f7"
      },
      "source": [
        "#explorando os dados\n",
        "criticas.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>texto</th>\n",
              "      <th>sentimento</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Eu fui e vi este filme ontem à noite depois de...</td>\n",
              "      <td>positivo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>O diretor do ator, Bill Paxton, segue sua prom...</td>\n",
              "      <td>positivo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Como um jogador de recreio com algum conhecime...</td>\n",
              "      <td>positivo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Eu vi esse filme em uma prévia, e é delicioso....</td>\n",
              "      <td>positivo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bill Paxton levou a verdadeira história do gol...</td>\n",
              "      <td>positivo</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               texto sentimento\n",
              "0  Eu fui e vi este filme ontem à noite depois de...   positivo\n",
              "1  O diretor do ator, Bill Paxton, segue sua prom...   positivo\n",
              "2  Como um jogador de recreio com algum conhecime...   positivo\n",
              "3  Eu vi esse filme em uma prévia, e é delicioso....   positivo\n",
              "4  Bill Paxton levou a verdadeira história do gol...   positivo"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kdyx7KB0lBmc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "1ba31322-d7f0-495a-aaa1-cc909d268990"
      },
      "source": [
        "#explorando os dados\n",
        "criticas.tail()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>texto</th>\n",
              "      <th>sentimento</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>49454</th>\n",
              "      <td>No final do filme, senti que era muito técnico...</td>\n",
              "      <td>negativo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49455</th>\n",
              "      <td>Este é o tipo de filme que meus inimigos me as...</td>\n",
              "      <td>negativo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49456</th>\n",
              "      <td>Eu vi Descent na noite passada no Stockholm Fi...</td>\n",
              "      <td>negativo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49457</th>\n",
              "      <td>Alguns filmes que você escolhe por um quilo sã...</td>\n",
              "      <td>negativo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49458</th>\n",
              "      <td>Este é um dos filmes mais idiotas que já vi. E...</td>\n",
              "      <td>negativo</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   texto sentimento\n",
              "49454  No final do filme, senti que era muito técnico...   negativo\n",
              "49455  Este é o tipo de filme que meus inimigos me as...   negativo\n",
              "49456  Eu vi Descent na noite passada no Stockholm Fi...   negativo\n",
              "49457  Alguns filmes que você escolhe por um quilo sã...   negativo\n",
              "49458  Este é um dos filmes mais idiotas que já vi. E...   negativo"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVs1ZXOctRHh"
      },
      "source": [
        "#dividindo o conjunto de dados em 20% para teste e 80% para treino\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "criticas_treino, criticas_teste = train_test_split(criticas,\n",
        "                                                   test_size=0.2,\n",
        "                                                   random_state=10)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdhtOW1qw_8A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "72fcc727-8e24-461c-8195-9189f8836541"
      },
      "source": [
        "# observando os dados para treinamento\n",
        "criticas_treino.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>texto</th>\n",
              "      <th>sentimento</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>19425</th>\n",
              "      <td>Tornado humano de 1976 é, em muitos aspectos, ...</td>\n",
              "      <td>positivo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22621</th>\n",
              "      <td>Efeitos surpreendentes para um filme deste tem...</td>\n",
              "      <td>positivo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42506</th>\n",
              "      <td>Este é um filme japonês, mas há um pouco de In...</td>\n",
              "      <td>negativo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6595</th>\n",
              "      <td>Eu não sou muito para filmes de \"policial\", ma...</td>\n",
              "      <td>positivo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41850</th>\n",
              "      <td>... Ruim em ser intencionalmente ruim ... Esta...</td>\n",
              "      <td>negativo</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   texto sentimento\n",
              "19425  Tornado humano de 1976 é, em muitos aspectos, ...   positivo\n",
              "22621  Efeitos surpreendentes para um filme deste tem...   positivo\n",
              "42506  Este é um filme japonês, mas há um pouco de In...   negativo\n",
              "6595   Eu não sou muito para filmes de \"policial\", ma...   positivo\n",
              "41850  ... Ruim em ser intencionalmente ruim ... Esta...   negativo"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a6hxEjdxEOQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faea80d6-c548-4338-eb1c-90bba2a4cc68"
      },
      "source": [
        "# observando os dados para treinamento\n",
        "len(criticas_treino)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39567"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnCbKOl2xJch",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2987e4d9-070d-4b8d-a5e3-6132f14a0b67"
      },
      "source": [
        "# observando os dados para teste\n",
        "len(criticas_teste)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9892"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMK00xVUxMjw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdf5105d-2b95-4071-b1db-70e65cd01edc"
      },
      "source": [
        "#importando os vetores do NILLC para executar a vetorização do corpo de texto\n",
        "!unzip \"/content/drive/My Drive/PLN/cbow_s300.zip\""
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/My Drive/PLN/cbow_s300.zip\n",
            "  inflating: cbow_s300.txt           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDq3g0TmxW-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "728c3597-d679-4819-dcaf-0230ee64c093"
      },
      "source": [
        "#lendo e observando os vetores\n",
        "with open(\"cbow_s300.txt\") as iteradorArquivo:\n",
        "  for contadorLinha in range(10):\n",
        "    print(next(iteradorArquivo))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "929606 300\n",
            "\n",
            "</s> -0.001667 -0.000158 -0.000026 0.001300 -0.000796 0.001527 0.000046 0.000584 0.000449 -0.000100 0.000353 0.001251 0.001069 0.000506 0.000574 0.000838 -0.000930 -0.001220 0.000317 0.001315 -0.001120 0.001373 -0.000040 -0.001580 0.000421 -0.000667 -0.001556 -0.000746 0.001604 0.001157 -0.000027 0.000354 0.000358 -0.000527 -0.000573 -0.001512 -0.001557 -0.001637 0.001617 -0.001511 -0.001022 -0.001426 0.001086 -0.001033 0.000593 0.000724 0.000627 -0.000450 -0.001140 0.000333 0.000524 0.001541 0.000284 0.000617 -0.000807 -0.000088 -0.000364 0.001126 -0.001230 -0.001138 -0.001280 0.001330 0.001257 0.000576 0.000764 0.000684 0.001008 -0.000215 -0.000629 -0.001228 -0.001557 -0.000311 -0.000246 0.000045 0.001136 -0.000645 -0.000549 0.001099 0.000858 -0.000886 0.000553 0.000303 0.001433 0.000732 0.001321 -0.000894 -0.000700 -0.000661 -0.001484 -0.000950 -0.001556 -0.000809 0.000348 -0.000068 0.000724 -0.000569 -0.000161 -0.001628 -0.001437 -0.000259 -0.000296 -0.001571 0.000149 0.000847 0.000613 0.000802 0.001507 0.001015 0.000377 0.000255 -0.000458 -0.000777 -0.001561 0.001601 -0.001520 -0.001210 0.000106 0.000714 0.000392 0.001311 -0.001192 -0.000090 -0.001097 0.000424 -0.000954 -0.001272 -0.001178 0.000036 -0.000181 0.000331 -0.001453 -0.001488 -0.001033 -0.000377 0.000257 -0.001418 0.001109 0.000722 0.000936 -0.000113 0.001215 -0.000263 0.000652 0.001190 -0.000258 0.001391 0.001213 0.000783 -0.001202 0.000470 -0.000879 0.000688 -0.001163 -0.001105 0.001497 0.001304 -0.001322 -0.001501 0.001377 0.001439 0.000884 0.000484 0.001239 -0.001578 0.000981 -0.000318 -0.001180 -0.001375 -0.001491 0.001057 -0.001028 0.000893 0.001028 0.000772 0.001636 -0.000331 -0.000247 -0.001006 -0.000329 0.000837 0.000605 -0.000959 0.001410 0.000488 0.001167 -0.000293 -0.001188 -0.000001 0.001135 0.001141 0.001504 0.000198 -0.001060 0.001551 -0.000003 -0.001474 -0.000391 -0.000880 0.000433 -0.000976 -0.001417 0.000563 -0.001188 0.000593 0.001584 -0.001602 -0.000439 -0.001148 -0.001256 0.001185 -0.000738 0.001543 -0.000846 -0.001029 -0.000641 -0.001587 0.001439 -0.001251 0.000942 -0.001414 -0.001106 0.001087 -0.000027 0.000757 -0.000159 -0.001014 -0.000891 0.000024 -0.000238 0.000157 -0.001067 0.000902 -0.001050 -0.000428 -0.001606 -0.000988 0.001391 0.001165 -0.000113 -0.001000 -0.000055 -0.001369 0.000684 0.000715 0.001407 0.000613 0.001389 0.001315 -0.000130 -0.001044 0.000175 -0.000035 0.000959 -0.000345 0.001209 -0.001251 -0.001219 0.001231 -0.000996 -0.001388 0.001038 0.001336 -0.001066 -0.000881 -0.001066 -0.001466 -0.000274 0.000201 0.000401 0.000132 0.000588 0.000589 -0.000128 0.001073 0.001197 0.000109 0.000770 0.001221 0.000996 -0.001174 0.000135 -0.001134 -0.001385 -0.000311 -0.001631 -0.000564 0.001162 -0.000322 -0.000469 0.001312 -0.001402 0.000239 0.000184 0.001300 0.000021 -0.001065 0.000047 -0.000301 0.001336 0.000332\n",
            "\n",
            ", -0.061483 -0.094368 -0.008557 -0.034702 0.021108 -0.011873 -0.041133 -0.095925 0.034668 -0.085286 0.076174 0.003314 0.019222 -0.038695 -0.008963 0.053399 0.162935 0.050372 -0.020163 -0.027230 0.061531 0.060840 0.074610 -0.056173 0.007621 -0.055220 0.018008 0.026096 0.033154 -0.082612 -0.081761 0.164978 -0.034423 0.003094 -0.018217 -0.087445 -0.074446 -0.000142 0.004218 0.036585 0.016095 -0.129141 -0.119698 -0.053717 0.005053 -0.114520 -0.017219 0.023693 -0.024115 0.053125 0.024658 -0.037689 0.012078 0.112701 0.028037 0.047618 -0.024196 0.050112 -0.073095 -0.090859 -0.030613 -0.109599 0.037756 0.063827 0.022537 -0.029640 -0.016311 0.021875 0.064882 0.039002 -0.082970 -0.043166 0.013695 -0.043153 -0.082203 -0.020087 -0.100360 0.007033 -0.074208 -0.067789 -0.024897 -0.020358 0.041731 0.101332 0.020217 -0.032473 0.087827 -0.033611 -0.150526 -0.016615 0.021147 -0.025058 0.097833 0.065067 0.051287 -0.079191 0.089563 -0.008436 -0.038352 0.019787 -0.058452 -0.009696 -0.051077 -0.112245 0.024886 -0.015172 -0.129670 0.068672 0.068483 0.009049 0.007055 -0.032763 0.001527 0.054264 0.029924 -0.023482 0.047470 0.008044 0.014534 0.071155 0.016700 -0.027491 -0.155782 0.039370 0.116605 -0.001262 -0.026638 -0.067078 0.078015 -0.066153 -0.039303 0.009535 -0.055698 -0.022250 0.046948 0.053810 0.038096 0.032157 -0.075257 0.008125 -0.034598 -0.020667 -0.003153 0.032491 -0.031064 0.030744 -0.049023 -0.046149 0.000792 0.010385 -0.057119 -0.122554 0.003210 -0.054363 -0.100899 0.069873 -0.009758 0.055455 0.049243 0.008346 -0.016087 0.093572 0.024125 0.058736 0.037243 -0.007478 0.032175 -0.054205 0.008798 0.032326 0.028384 -0.032259 -0.041842 -0.058281 -0.025145 0.011097 0.023598 -0.033376 0.026204 0.032505 -0.009283 0.041076 0.055565 -0.081757 -0.010077 0.058251 -0.014379 -0.040951 0.006938 -0.004179 0.052006 -0.063725 0.035674 -0.066554 0.030910 -0.004032 0.077445 0.029495 -0.064931 0.040263 -0.055423 -0.021571 0.086767 -0.003583 0.073308 -0.043991 0.022503 -0.028692 -0.063626 -0.048238 0.013439 -0.043673 -0.101352 -0.004321 0.125507 0.088486 0.042756 -0.014497 -0.053445 0.021800 0.038406 -0.034023 -0.074428 -0.132825 0.082152 -0.068497 0.004738 0.047527 -0.073890 0.051089 -0.055886 -0.047786 0.040247 -0.053966 -0.015752 0.099451 0.008218 -0.010716 -0.031540 0.036168 0.054244 0.051809 0.035158 0.043006 -0.027902 0.000130 0.103397 -0.114831 -0.036648 -0.036143 0.024432 0.084740 0.001801 0.044475 -0.035746 -0.024109 0.051210 -0.025769 0.016073 -0.000351 -0.029183 -0.075292 0.042163 0.025010 -0.041439 -0.059192 0.026617 -0.040852 0.034697 0.014691 -0.057382 0.046141 0.070360 0.045274 0.065880 0.011023 -0.031292 -0.015784 -0.023421 -0.042788 0.019669 0.035010 0.036188 -0.058060 -0.093562 0.030321 -0.054753 0.097162 0.001134 0.018939 -0.150218 -0.009928 0.051118 0.105212 -0.055051 -0.047959 -0.136800 -0.003198 0.068969 -0.022456\n",
            "\n",
            "de -0.232068 0.066729 0.103946 -0.072608 0.126237 -0.004782 -0.025139 -0.141489 -0.069438 -0.071078 0.175772 -0.017257 0.094824 0.011020 0.029226 -0.010670 0.144973 0.105333 -0.088273 -0.070952 0.054747 -0.048955 -0.047809 -0.030763 -0.052293 -0.003596 0.078465 0.144430 0.129697 -0.078427 -0.025080 0.212887 0.119806 0.101703 -0.142488 -0.031272 -0.026594 -0.109429 -0.154688 -0.121492 -0.103781 -0.177948 -0.125544 -0.120136 0.031658 0.054160 0.060493 -0.115676 0.132511 -0.001668 -0.125569 -0.124120 0.029025 0.006200 -0.007915 0.058489 0.122710 0.004660 -0.082200 0.123740 0.052747 -0.065657 0.154747 0.155071 0.003547 -0.154675 -0.035110 0.119117 0.080579 0.044262 0.048753 0.054975 -0.064624 -0.046866 0.028242 0.144197 0.029916 -0.024569 0.195781 -0.028608 -0.008233 -0.032542 0.012042 0.011934 -0.068173 0.037028 0.018363 -0.038244 -0.091176 -0.019795 0.051640 -0.008025 -0.025825 -0.001686 -0.069845 -0.039987 0.066244 0.088229 -0.171358 -0.030783 0.082344 0.059267 0.000550 -0.216285 -0.201356 -0.131063 -0.033069 -0.057126 -0.127576 -0.099251 -0.044669 0.035363 -0.119503 -0.090019 -0.006226 -0.000546 0.020731 0.016140 -0.007438 0.032885 -0.161048 0.105681 -0.115705 -0.078013 0.102293 -0.107958 -0.025926 -0.067986 0.017502 -0.054865 -0.007930 -0.006999 -0.000949 -0.026097 0.106295 0.115368 -0.033742 -0.046025 0.009641 -0.065890 0.198822 0.078333 0.062547 0.044947 -0.098424 0.140951 0.109045 -0.004204 -0.174950 0.034234 0.040793 -0.210289 0.104861 0.083616 -0.042511 -0.031439 -0.113108 0.000401 -0.171665 0.097890 0.008065 0.080684 -0.009576 -0.042125 0.177337 -0.005585 -0.004165 0.069755 -0.100323 -0.023289 0.039458 -0.089292 0.069543 -0.009576 0.011562 -0.076654 0.009681 -0.018808 -0.047881 0.026709 -0.021200 -0.032544 0.100362 0.037157 0.005169 -0.001280 -0.064330 -0.049024 -0.002354 0.004443 0.049129 0.026813 0.034249 0.068827 -0.038583 -0.116914 -0.107378 0.046983 0.038218 -0.082186 -0.124208 0.066872 -0.081745 -0.016516 0.016110 0.046844 0.176223 0.083604 -0.086893 -0.114739 -0.159588 0.007700 0.004887 0.006024 -0.020026 0.045816 0.033604 0.054474 0.089348 -0.024353 0.104835 0.084334 0.052662 -0.041422 -0.027877 0.002816 0.150068 -0.052310 0.017154 -0.152326 0.067753 -0.082644 0.119430 -0.012345 0.082965 -0.005791 -0.082770 -0.030068 -0.037331 -0.075347 0.035060 -0.092023 -0.001051 0.012675 0.128757 -0.048579 -0.078527 -0.134126 -0.060009 0.096834 -0.045947 0.132404 -0.030576 -0.006618 -0.088179 -0.124597 -0.095311 -0.086943 0.007010 0.059925 -0.077005 -0.035458 -0.017957 0.081104 0.060141 0.152996 0.083737 -0.025909 0.005420 -0.006300 -0.075839 -0.012399 -0.001624 0.039090 -0.040755 -0.013051 -0.072733 -0.048062 -0.082573 -0.013851 -0.120222 0.011452 -0.083538 0.015996 -0.110800 0.012405 0.045823 0.026705 -0.040789 0.064309 0.007381 -0.037854 0.076050 0.104702 0.010307 -0.103478 0.085227 0.064233 -0.015908 -0.047998\n",
            "\n",
            ". 0.027867 0.077901 -0.054738 -0.095938 -0.010536 0.015269 -0.005752 -0.048440 -0.104021 0.054583 0.050108 0.054979 0.071112 -0.056665 -0.140868 0.203626 0.211330 0.137270 -0.038069 -0.108607 0.060027 0.052436 0.126758 -0.078212 0.008864 -0.028087 0.029884 0.114899 -0.058657 -0.166441 -0.188866 0.294844 -0.147703 -0.095883 -0.104021 -0.097608 -0.172317 0.035795 -0.019025 0.057503 -0.030564 -0.125182 -0.257359 -0.106566 -0.002708 -0.035769 -0.072053 -0.005640 0.016920 0.155101 -0.226088 -0.070904 -0.110280 -0.075773 -0.190123 0.197397 -0.108603 0.155954 0.096544 -0.183301 0.008154 -0.195997 0.035284 -0.025561 0.159142 -0.141005 0.140643 -0.160588 0.140709 0.059974 -0.012396 -0.074953 -0.038325 -0.165941 -0.226005 -0.043738 0.052905 0.085072 -0.165731 0.016894 -0.136569 0.088165 -0.032044 0.022789 0.051207 0.015907 0.093662 0.037576 -0.286139 -0.107587 0.165001 0.095293 0.181427 0.038072 0.078180 -0.116599 0.138464 0.138386 0.020201 -0.028087 0.038829 -0.040074 -0.044944 -0.179895 0.035044 0.090347 -0.036088 0.154828 0.090421 0.172175 -0.127294 0.060227 0.164694 0.141367 0.049326 -0.136940 0.060633 -0.089581 -0.069639 0.043680 0.141455 -0.060588 -0.118859 0.181506 -0.051302 0.165763 0.151366 -0.191232 -0.103888 -0.077344 -0.016534 0.073607 -0.008464 0.216368 0.038251 0.002552 -0.112613 -0.049596 0.018040 -0.032907 -0.131335 -0.049530 0.057485 0.129446 -0.017884 0.075667 -0.061577 -0.015305 0.007649 0.116636 -0.132568 -0.244250 -0.048748 0.003139 -0.123311 0.018462 0.126955 0.079116 -0.067653 0.019126 0.044182 0.037610 0.199809 -0.118487 0.011276 -0.061652 -0.034426 -0.120148 -0.171490 0.009182 0.182921 -0.165212 0.149898 0.062080 -0.129040 0.051808 -0.075070 -0.068688 0.018707 -0.008050 0.108429 -0.111089 0.021230 -0.125982 0.077740 0.140251 -0.051900 0.036107 0.019580 -0.084050 0.065747 0.015718 -0.016900 -0.026964 0.126565 -0.087491 0.069504 0.108362 -0.006355 0.118521 -0.002510 0.060457 0.082848 0.000294 -0.037938 -0.026097 0.084446 0.015568 -0.050540 -0.117080 0.100227 -0.046050 -0.059615 0.086632 0.097266 0.084578 -0.036891 -0.033246 0.021302 -0.149581 -0.006353 -0.035713 -0.177163 -0.016629 -0.028634 -0.147015 -0.017861 0.009931 -0.181849 0.065564 -0.007616 0.094861 0.089787 0.005743 -0.127185 0.093027 -0.107247 -0.019161 -0.201082 -0.008504 0.126500 0.034318 0.011876 0.012696 -0.090981 0.012814 0.038388 -0.274763 -0.123303 -0.073078 0.027702 0.035841 -0.080954 0.091305 0.029009 -0.006000 0.121087 0.016678 -0.070181 -0.080839 -0.016332 -0.081390 -0.059594 0.131390 0.039603 -0.222503 -0.039382 -0.071041 0.267399 -0.010011 -0.077326 -0.044107 0.052901 0.023623 -0.087243 0.011539 -0.006381 -0.074194 -0.041591 -0.061872 -0.098552 -0.094357 -0.062879 -0.150341 -0.087596 0.330078 0.028435 0.087784 0.034434 -0.008122 -0.258485 -0.109828 0.013279 0.043340 0.136803 -0.081573 -0.136733 -0.081884 0.306241 0.093106\n",
            "\n",
            "a -0.019764 -0.096043 -0.010960 0.102012 -0.101848 -0.010257 0.004692 -0.165735 -0.179723 0.067659 0.037108 0.100931 -0.211779 0.163603 -0.042376 -0.021683 -0.014900 0.101581 -0.053583 0.065435 0.126251 0.062320 -0.057445 -0.107130 -0.037044 -0.072958 -0.066655 -0.067962 -0.070879 -0.187941 0.065208 0.102302 -0.024876 0.151433 0.012228 -0.065570 -0.063793 0.015519 0.046230 0.174824 -0.100376 -0.081244 -0.039337 0.017851 0.125168 -0.096404 -0.133070 -0.023451 0.117965 0.005949 0.038928 0.060488 -0.014678 0.024891 -0.001455 0.048539 0.133535 0.084564 0.018362 -0.054294 -0.235691 -0.151528 0.043101 -0.097685 -0.034775 -0.127507 0.010346 -0.010458 0.047631 0.002045 -0.013826 -0.139644 0.128220 -0.020997 -0.057176 0.007458 -0.033133 -0.083235 -0.075585 -0.079414 -0.023531 0.076794 0.051968 0.026857 0.007574 -0.039602 0.027458 -0.146083 0.008662 0.113647 0.007963 0.023413 0.088046 -0.079530 0.027995 -0.017113 -0.056750 0.069447 0.094213 0.123728 0.010051 -0.054795 0.041003 -0.044294 -0.066911 -0.025221 0.059618 0.023494 -0.095354 0.029266 0.047979 0.079102 -0.061757 0.116363 0.011112 0.136784 -0.027020 -0.002793 -0.092629 -0.047217 0.111263 0.102819 0.090932 -0.085343 -0.026490 -0.033315 -0.028970 -0.118673 -0.067360 -0.207183 -0.076011 0.017469 -0.078908 0.031023 0.090820 -0.054443 0.062466 0.065952 -0.028265 0.051329 0.090671 0.106923 0.028011 0.014720 -0.036025 0.099207 -0.013510 -0.147551 -0.077657 -0.099269 0.001171 0.059405 -0.006946 0.123997 0.023131 0.102308 -0.040784 0.027455 -0.181898 0.095958 0.155467 -0.077250 -0.124941 0.058529 0.035851 -0.004447 0.035127 -0.024015 -0.029101 -0.107558 0.055650 -0.017027 0.008997 -0.016504 0.018768 -0.080671 0.029003 0.013937 0.002876 -0.064262 0.001246 0.073494 -0.013986 -0.100303 0.027376 0.030375 -0.177618 0.016413 -0.008711 -0.257308 -0.031904 0.068326 0.038748 0.006762 -0.045245 0.038158 0.093055 -0.069432 -0.140442 -0.030486 -0.097529 0.251017 -0.059000 -0.037363 -0.020030 0.116937 0.018191 -0.005464 0.016485 0.009930 0.007318 -0.131821 -0.005316 -0.063905 0.066936 -0.007235 0.066709 0.153262 -0.087015 -0.196866 0.042973 0.124447 0.034619 -0.021113 0.029881 -0.072679 0.068707 -0.033823 -0.140311 -0.158137 0.215081 -0.020170 -0.062018 -0.038955 0.158768 -0.155656 0.016471 -0.071324 0.069629 -0.070692 0.114725 0.114054 0.154133 -0.143244 -0.082015 -0.070578 0.094558 -0.073491 -0.084676 0.089027 -0.053428 0.034783 0.007488 0.008849 0.137206 -0.061942 -0.155206 -0.022297 -0.054165 0.206418 0.058566 0.033847 0.039555 0.042072 -0.003160 0.350700 -0.067416 -0.089351 0.147734 0.026404 -0.128322 -0.143885 -0.189820 0.162824 -0.038651 -0.045660 0.083507 0.118510 -0.054740 -0.053446 -0.106545 -0.087144 -0.159905 0.006153 0.057539 0.079486 -0.015659 -0.014220 -0.013549 -0.122177 -0.154875 0.319072 -0.251233 0.070468 -0.084041 -0.067580 -0.163386 -0.026553 -0.033245 -0.035199\n",
            "\n",
            "o 0.050016 0.094213 -0.234393 0.057870 -0.255710 -0.099076 0.162541 -0.073166 -0.072231 0.087253 0.154162 -0.021438 0.095246 0.086680 0.027481 0.055959 0.176635 -0.158901 -0.076106 -0.036695 0.115689 -0.039255 -0.116532 -0.143528 0.059030 0.049429 -0.009721 0.204578 -0.020946 -0.254149 -0.167956 0.024087 -0.094412 -0.066287 -0.085987 -0.081115 -0.035314 -0.000736 0.069831 -0.034319 -0.041741 0.007945 0.106777 0.119971 -0.051358 0.152951 -0.109669 0.102599 0.102598 0.044424 -0.088313 0.142374 0.042868 -0.042467 0.199348 -0.234265 -0.044845 -0.109492 0.005783 -0.045873 -0.049017 -0.037446 0.105329 -0.163747 -0.139140 0.153179 -0.117726 -0.113436 0.010813 0.097110 0.074618 0.041083 0.082912 -0.052041 0.009895 0.160828 0.058190 -0.215907 -0.024543 0.042466 -0.011712 0.064930 0.097498 -0.080108 0.019837 -0.059179 0.077248 -0.135312 0.132748 0.069083 0.075949 -0.232743 0.078279 -0.078944 0.116486 0.004099 0.057786 -0.125938 0.068976 0.176170 0.044454 0.085901 0.033849 0.057793 -0.083662 0.061258 -0.037814 0.050392 -0.067302 -0.102103 -0.011153 0.029286 0.032976 0.003400 0.061664 0.076337 -0.097293 0.026432 -0.059397 -0.160978 0.069399 0.307550 0.069981 -0.122410 0.003086 0.027841 0.134213 -0.018279 -0.025608 -0.156602 0.062460 -0.051757 0.030337 -0.042270 0.007516 0.055478 0.075345 0.112685 -0.047181 0.001009 0.031433 -0.134699 -0.034702 -0.082685 -0.182748 -0.097813 0.053774 -0.067876 -0.090774 -0.035145 0.080364 -0.242563 0.024905 -0.012293 0.110051 0.103592 0.138403 -0.035121 0.037955 -0.028042 0.005687 0.042462 0.045434 0.127590 -0.049950 0.079609 -0.152268 -0.102837 -0.083840 -0.132681 -0.073176 0.031672 0.104991 0.082840 0.142188 -0.047643 0.054661 -0.112325 0.006551 0.079335 0.011705 -0.042121 0.086597 0.088994 0.079949 -0.002677 0.106816 -0.055035 0.068153 -0.167192 0.009149 -0.028594 -0.039560 0.001607 -0.122615 0.106024 0.107250 0.050540 -0.043291 0.061397 0.001380 0.054964 0.040893 -0.045046 0.002312 0.032035 -0.026698 -0.042064 0.144622 0.076241 0.024255 -0.086833 0.098275 -0.083894 0.086363 -0.004560 -0.029056 0.126784 0.071785 -0.025692 0.000966 0.100620 0.030299 -0.163968 0.042070 -0.198341 0.099603 -0.113758 0.124685 -0.069966 -0.165131 -0.134782 -0.020419 -0.112747 0.123515 -0.045274 0.167949 -0.109681 -0.146224 0.075453 0.054352 0.023153 -0.062161 -0.059457 -0.124327 0.041855 0.205124 -0.083616 -0.062827 0.017066 0.025000 0.079311 0.018874 -0.055492 -0.129246 0.099183 -0.022749 -0.079510 -0.130005 0.012160 0.060842 0.018816 -0.180103 0.057191 0.018971 0.216519 -0.003726 0.155861 0.112271 0.091379 -0.073707 -0.059254 0.011068 -0.005690 -0.098677 -0.082454 0.105838 0.042144 0.100894 0.096960 -0.228625 0.064737 0.022971 -0.001549 -0.105277 0.149519 -0.055187 0.201671 0.098550 0.056229 -0.107189 -0.005223 -0.077126 0.020865 -0.098107 0.064205 -0.174567 0.196321 0.233813 0.038634\n",
            "\n",
            "e -0.168088 -0.001531 0.017038 -0.137315 -0.026399 -0.018215 0.026076 0.019779 -0.095212 -0.090352 -0.106060 0.061975 -0.000318 -0.002945 0.119734 0.149804 0.108117 0.064784 0.065574 -0.038343 0.159915 -0.006124 -0.038307 -0.094810 -0.019356 -0.045115 -0.121859 0.028930 0.045017 -0.138268 0.099283 0.128659 0.077656 0.032982 -0.086139 0.051552 -0.077685 -0.066857 -0.045233 -0.185625 0.013728 -0.067088 -0.061796 0.091931 0.007931 -0.033784 -0.026604 0.026487 0.002281 0.000939 -0.043053 -0.082718 -0.101476 0.014253 0.106507 0.059285 -0.034097 0.032848 -0.067852 0.103892 0.020742 -0.049702 0.085734 0.069140 -0.036293 -0.001048 -0.043403 -0.033758 -0.041652 -0.049051 -0.070893 -0.002921 0.096254 -0.046406 -0.016074 0.074894 0.098854 -0.015867 -0.024065 0.048818 -0.058226 -0.031364 0.055631 -0.001012 0.002966 0.017624 0.082472 -0.025849 -0.099736 -0.065253 0.002511 0.026550 -0.046750 0.005665 0.058496 -0.068569 0.110588 -0.003354 -0.133236 -0.140404 0.090284 -0.089065 0.010600 -0.143848 0.023044 0.035726 0.044885 -0.081761 0.020406 0.045142 -0.099682 0.013478 -0.102208 -0.048206 0.040736 0.012733 0.064284 0.052800 0.046179 -0.083605 -0.002064 0.003024 0.078588 -0.075731 -0.038192 -0.010662 -0.062210 -0.060302 0.005740 -0.119290 0.049789 -0.009878 0.033791 0.145393 -0.064118 0.005292 -0.046015 -0.012572 -0.002266 0.018877 -0.114718 0.043831 0.036031 -0.077071 0.079499 0.047130 0.136516 -0.029402 -0.095073 -0.198367 0.077792 -0.092858 -0.115212 0.064883 -0.083748 -0.115535 -0.017903 0.080732 0.088457 0.037072 0.044588 0.042071 0.064027 0.035189 0.071027 -0.028330 0.026847 -0.005846 -0.003655 -0.019447 0.057641 -0.057623 0.032575 -0.009248 0.103035 -0.087592 0.049022 -0.030181 0.055313 0.001550 0.136222 -0.081518 -0.003322 -0.196934 -0.125240 -0.039119 0.082201 -0.036932 -0.030666 0.101376 -0.008224 0.045433 0.055995 -0.108370 0.120019 0.024427 -0.055956 0.076503 -0.063588 -0.091926 -0.047620 0.071838 -0.050062 0.122101 0.011679 0.015549 0.060463 -0.174818 0.006131 -0.035602 -0.039755 0.044683 -0.002324 -0.096353 0.153145 -0.181759 0.059133 0.008242 0.009471 0.004472 -0.005789 0.035746 0.107733 0.019606 0.035323 0.013101 0.078772 0.031495 0.011914 0.035316 0.029965 0.010626 0.024444 -0.036373 0.041982 -0.059195 -0.062521 -0.032445 -0.050086 -0.002740 -0.033702 -0.052157 -0.100889 0.010753 -0.017195 0.076872 -0.114096 -0.134334 0.019812 -0.147168 -0.048026 0.055315 -0.021564 0.092953 -0.070098 -0.009133 -0.021408 -0.043442 -0.059603 -0.026196 -0.046402 -0.024446 0.036482 -0.021195 -0.014285 -0.003113 -0.053620 -0.057722 -0.005912 -0.228597 -0.022520 0.008453 -0.131441 0.074053 0.010990 -0.029988 -0.106588 -0.005512 0.036736 -0.002115 -0.078824 -0.067073 0.034071 -0.037642 0.019106 0.080103 0.068267 -0.052487 -0.070635 0.073949 -0.056421 0.017687 0.011800 -0.014045 0.006226 -0.014210 0.088889 -0.007613 -0.042151 0.091935\n",
            "\n",
            "que 0.105677 -0.012054 -0.134709 -0.013888 -0.080373 -0.084105 0.028820 -0.039461 -0.011410 0.033408 -0.031719 0.198935 0.047353 -0.090613 -0.098891 -0.010202 0.144315 0.151674 0.016502 -0.133896 0.031100 -0.007432 -0.059600 0.059022 -0.038658 0.167877 0.082268 0.217249 -0.072088 0.076910 0.079216 -0.037108 0.014821 -0.078732 -0.035230 0.024507 -0.126809 -0.017172 0.040998 -0.043692 -0.149663 -0.049520 0.024513 -0.063125 0.058306 -0.012108 -0.119330 0.034781 -0.102405 0.088640 -0.272843 -0.059915 -0.108119 -0.004685 0.184494 0.029357 -0.012836 0.087409 -0.093633 0.141491 -0.051050 -0.072004 -0.164353 -0.078488 -0.075351 -0.029836 -0.011346 -0.081588 0.032113 -0.120232 -0.028536 0.010188 0.169394 -0.077409 -0.026955 0.083380 0.010644 0.026978 0.131446 -0.055521 -0.142428 -0.164762 0.016372 -0.254786 -0.015511 -0.044090 -0.091434 0.042059 -0.182369 -0.057621 -0.113482 -0.031757 0.036860 0.035760 -0.084302 -0.244284 0.014317 -0.110019 0.076959 -0.003242 0.058425 -0.068168 -0.150191 -0.054872 0.067636 -0.130009 0.033544 -0.028672 -0.029417 -0.003639 0.008028 0.067533 0.015459 0.057868 -0.095344 0.023596 0.005216 -0.068982 0.110498 -0.045724 0.067068 -0.103614 -0.010749 0.079515 -0.160577 -0.037640 -0.090058 0.194754 -0.137685 0.048330 0.023402 -0.080694 -0.076719 0.081176 0.145217 -0.312484 -0.158713 0.003970 0.017241 0.020559 0.019566 -0.020686 0.027909 -0.088033 0.055311 0.114986 0.046477 -0.026289 -0.085503 0.072325 0.033167 -0.027302 0.043202 -0.125175 -0.046430 -0.089749 -0.052517 -0.026923 -0.157140 0.142859 -0.035472 0.053636 -0.015367 -0.041692 0.004743 -0.053162 -0.073490 -0.031711 0.009967 -0.034655 0.209237 -0.052801 0.121190 -0.048079 0.203458 0.037113 -0.095015 0.143972 0.046959 -0.014909 0.169301 -0.034984 0.006088 -0.180444 -0.098740 -0.082224 0.042493 -0.048264 0.034078 -0.155237 0.049812 0.108571 -0.103458 0.015055 -0.005921 -0.020380 -0.059664 0.063496 0.120677 0.028720 0.010933 -0.034094 -0.033195 0.065065 -0.150067 -0.127015 -0.115948 -0.168308 -0.079605 0.112220 -0.024935 0.008233 -0.050985 -0.047235 -0.041944 0.099358 -0.094031 0.008722 -0.044243 0.091927 0.029639 0.053425 -0.020329 -0.007264 0.029055 -0.027748 0.021123 0.023926 -0.068259 -0.034626 0.010976 -0.027882 0.074465 0.017960 -0.142224 0.010472 -0.005171 0.001003 0.031482 0.048073 0.122005 -0.012546 -0.127375 0.137744 -0.043389 -0.067031 -0.028339 -0.102349 -0.001278 -0.075125 -0.209901 0.022713 0.123317 -0.005108 0.206613 -0.119748 0.148568 0.189863 -0.005551 0.042991 0.024824 0.095960 0.049202 -0.115647 -0.075263 -0.101255 0.047671 0.006936 -0.147872 -0.048649 0.152138 0.090100 -0.054797 0.180955 -0.016752 -0.090723 -0.126502 0.137744 0.065590 -0.095213 -0.141592 -0.081258 0.061414 0.138765 -0.047507 0.155577 -0.004058 -0.054305 -0.064755 -0.155780 0.019290 0.092772 -0.113740 0.062508 -0.108115 -0.104516 0.233178 0.085033 -0.063993 0.107476\n",
            "\n",
            "do -0.180377 0.215497 -0.243985 -0.111583 -0.213779 -0.101028 -0.000539 -0.101256 -0.070914 0.038753 0.046168 -0.014838 0.180611 0.000883 0.060867 0.085324 0.226791 -0.021134 -0.137997 -0.202197 0.148571 -0.050565 -0.078147 -0.128638 -0.060884 0.050100 0.018876 0.214215 0.115665 -0.095654 -0.142162 0.120025 -0.097131 0.072397 -0.038019 0.105100 0.071983 -0.160280 -0.051489 -0.098811 -0.117858 -0.034939 -0.009614 0.107021 -0.047199 -0.005264 -0.000139 0.071706 0.161239 -0.161907 -0.337947 0.006058 0.022506 -0.037415 0.171261 -0.062566 -0.062829 -0.086872 0.049601 0.208633 -0.010851 0.096788 0.242130 -0.103836 -0.078473 -0.008937 -0.192728 0.135349 -0.090459 0.275743 0.134895 0.051969 -0.132400 0.040473 -0.095963 0.027610 0.029590 -0.188700 0.164033 0.032661 -0.109406 0.180492 0.147311 0.025806 -0.079043 -0.050046 0.065651 -0.066955 0.037583 -0.031008 0.012122 -0.225802 0.106807 0.067123 0.035571 -0.204494 -0.004813 -0.022719 -0.267100 0.241056 0.115854 0.094971 -0.032575 0.045279 0.030343 -0.103868 0.013372 0.008258 0.027818 -0.051606 0.060680 0.094947 -0.025672 -0.044170 -0.007713 -0.008499 -0.111320 0.112186 -0.091970 -0.085167 0.001894 0.156088 -0.067075 -0.027825 0.093561 0.070251 0.049796 -0.161722 -0.051384 -0.124694 -0.016951 -0.141747 0.002894 -0.133038 -0.015664 0.080839 -0.090980 0.084050 0.089413 0.021215 0.043560 -0.075679 -0.068300 -0.025250 -0.157061 0.022866 0.055717 -0.019705 0.036159 0.183264 0.073571 -0.227341 -0.020978 0.064482 0.075304 0.010691 -0.068159 0.110613 0.047416 -0.029155 -0.043249 -0.074944 0.011065 -0.047526 0.018594 0.114240 -0.265600 0.007173 -0.118795 -0.109128 0.000016 -0.025266 -0.101829 0.167882 0.163892 -0.112688 -0.195755 -0.162868 0.114179 0.149630 0.075342 0.024940 0.099575 0.154626 0.069327 0.046685 0.046306 -0.138379 0.070196 -0.071198 0.028745 0.052231 -0.034552 -0.065006 0.070413 -0.061854 0.102099 0.063467 -0.002194 0.041076 -0.095035 0.138216 0.123084 -0.022177 0.178009 0.171209 0.193143 -0.170747 0.013212 0.020312 -0.129422 0.079598 -0.068529 -0.147807 0.047131 -0.137531 0.091482 0.030125 0.145402 -0.080191 0.025542 0.178609 0.118927 -0.206678 0.094064 -0.042739 -0.084631 -0.076197 0.253984 -0.273463 -0.026424 0.022400 0.012012 0.061563 0.131845 0.000155 -0.068048 -0.138741 -0.137770 0.143289 -0.017924 -0.111125 -0.055598 -0.012842 0.040329 0.034049 0.148260 -0.121714 -0.091318 0.030290 0.127014 0.037068 -0.070445 -0.136299 -0.232076 -0.028718 0.111683 -0.116630 -0.077730 0.033543 -0.032343 0.091454 0.094135 0.163275 0.123356 0.155053 0.137251 0.054886 0.086155 0.132626 -0.112011 0.103465 -0.085405 -0.075092 0.038307 0.075783 0.028715 0.020250 0.078956 0.012173 -0.058316 0.084967 -0.099896 -0.090466 -0.126150 0.144591 0.025099 0.059846 0.102206 0.092871 -0.004201 -0.062104 0.143311 -0.046741 0.010705 -0.065122 -0.074470 0.072302 0.144289 0.067299\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3X4vo78xf6i"
      },
      "source": [
        "#importando  os modelos da biblioteca gensim para vetorização com os vetores do NILLC\n",
        "#implementando o modelo com o word2vec,e os vetores de 300 dimensões, no método CBOW\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "modelo = KeyedVectors.load_word2vec_format(\"cbow_s300.txt\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUPCrLwRxh5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32192caf-3f16-4cf8-ed30-4ad34bfb1eea"
      },
      "source": [
        "#importando a biblioteca string e o NLTK punkt para tokenização do corpo de texto\n",
        "import string\n",
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAKA5Npb0eES"
      },
      "source": [
        "#importando numpy para operar com os vetores\n",
        "import numpy as np\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxCf4y5QzXKE"
      },
      "source": [
        "#função para tratamento dos dados e tokenização do corpus de texto\n",
        "def tratarTokenizar(texto):\n",
        "  texto=texto.lower()\n",
        "  tokens_validos = []\n",
        "\n",
        "  for token in nltk.word_tokenize(texto):\n",
        "    if token not in string.punctuation:\n",
        "      tokens_validos.append(token)\n",
        "\n",
        "  return tokens_validos"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVQFzw80zj5x"
      },
      "source": [
        "#função para vetorização\n",
        "def vetoresTextos(textos):\n",
        "  x = len(textos)\n",
        "  y = 300\n",
        "  vetores = np.zeros((x,y))\n",
        "\n",
        "  for itexto in range(x):\n",
        "    tokens = tratarTokenizar(textos.iloc[itexto])\n",
        "    vetores[itexto] = somaVetores(tokens)\n",
        "\n",
        "  return vetores"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqcQM2TZ0s6i"
      },
      "source": [
        "#função para somar os vetores frutos da tokenização\n",
        "def somaVetores(tokens):\n",
        "  vetorSomado = np.zeros(300)\n",
        "  for token in tokens:\n",
        "    try:\n",
        "      vetorSomado += modelo.get_vector(token)\n",
        "    except:\n",
        "      if token.isnumeric():\n",
        "        token = \"0\"*len(token)\n",
        "        vetorSomado += modelo.get_vector(token)\n",
        "      else:\n",
        "        vetorSomado += modelo.get_vector(\"unknown\")\n",
        "\n",
        "  return vetorSomado"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is4QEL2B0IqZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0fef923-8e7c-4f6b-d63d-da2fe0accb8f"
      },
      "source": [
        "#gerando vetores de treino e vetores de teste, observando o formato e dimensões dos vetores\n",
        "vetores_treino = vetoresTextos(criticas_treino.texto)\n",
        "vetores_teste = vetoresTextos(criticas_teste.texto)\n",
        "print(vetores_treino.shape)\n",
        "print(vetores_teste.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(39567, 300)\n",
            "(9892, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSq-47YP-Bl3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10d137bf-bb12-46ea-99d1-d2d71ec82fa9"
      },
      "source": [
        "#observando vetores de treino\n",
        "print(vetores_treino)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2.73421795  0.311943   -3.1896231  ...  1.24842096  0.65088403\n",
            "   1.60167697]\n",
            " [-0.80182408  1.91213906 -2.71754403 ...  2.95214997  1.79892198\n",
            "  -1.72515507]\n",
            " [ 4.75925199  2.06169404 -2.56801903 ...  1.60172105 -1.86328699\n",
            "   1.63948605]\n",
            " ...\n",
            " [ 2.30013693  1.71708792 -1.03630602 ...  1.56657088  2.42961\n",
            "  -2.98386602]\n",
            " [ 4.34052694  4.23035909 -6.73330402 ... 11.77689588 -0.45343395\n",
            "   2.31720613]\n",
            " [ 3.08371994  0.46220803 -3.14173901 ...  1.47123488  1.171558\n",
            "   3.11910319]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhY8a0s_-HCa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5c7d254-7cbe-4c6c-9a8c-c840616740a3"
      },
      "source": [
        "#observando vetores de teste\n",
        "print(vetores_teste)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 11.50125596   0.38926304 -16.14189713 ...  12.3522879    0.71080703\n",
            "   -1.07189195]\n",
            " [  3.543043    -0.338089    -3.38593103 ...   4.40340801   0.35946697\n",
            "    1.28489399]\n",
            " [ 12.03879392   6.542755   -11.0890261  ...  13.00769784   2.86570517\n",
            "    1.58576021]\n",
            " ...\n",
            " [  4.12080099  -0.40596702  -0.93938813 ...   1.77653997   1.21599804\n",
            "   -0.20062188]\n",
            " [  2.22691678   0.45447803 -15.69204017 ...  -1.94338914   7.60645417\n",
            "   -4.27185499]\n",
            " [  4.26931801   3.35502904  -4.29404205 ...   5.02443592   1.12545601\n",
            "   -1.86832695]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFcSGoIp1Gka"
      },
      "source": [
        "#classificador de Regressão Logística\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classificador = LogisticRegression()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_EZgn0A0ZOr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c14b61d8-587e-4989-bd94-b36ebf21be1e"
      },
      "source": [
        "#implemnentando a classificação\n",
        "classificador.fit(vetores_treino, criticas_treino.sentimento)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=200,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ntBwS1F1ZET",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e6170e2-8d6e-4931-d724-65d05885cca3"
      },
      "source": [
        "#apresentando o score de classificação de 80.50%\n",
        "classificador.score(vetores_teste, criticas_teste.sentimento)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8049939344925192"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZfUVLjTAlal",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd86fd77-0981-4113-a6e9-b8a9691ffd8e"
      },
      "source": [
        "# as categorias previstas só poderiam ser 2 conforme o dataset, positivo e negativo\n",
        "categorias_previstas = classificador.predict(vetores_treino)\n",
        "categorias_previstas"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['positivo', 'positivo', 'negativo', ..., 'negativo', 'negativo',\n",
              "       'positivo'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDSeUmDcDyAH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d66d0e9-b607-4fc5-ceed-964f341f39b5"
      },
      "source": [
        "#enumerando quantas foram as categorias previstas\n",
        "len(categorias_previstas)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39567"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqFm10qjD47h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37f7c9df-10db-45a9-f7ab-462af46e4e0d"
      },
      "source": [
        "#elaborando o relatório de classificação, com 81% de precisão e 81% de recall\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "categorias_previstas = classificador.predict(vetores_teste)\n",
        "relatorio = classification_report(criticas_teste.sentimento, categorias_previstas)\n",
        "print(relatorio)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negativo       0.81      0.81      0.81      4991\n",
            "    positivo       0.80      0.80      0.80      4901\n",
            "\n",
            "    accuracy                           0.80      9892\n",
            "   macro avg       0.80      0.80      0.80      9892\n",
            "weighted avg       0.80      0.80      0.80      9892\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_0v_itUEPrO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f106e17-b98a-4100-ea83-788b3851f33d"
      },
      "source": [
        "#importando os vetores de 300 dimensões para word embedding com o método skip-gram\n",
        "!unzip \"/content/drive/My Drive/PLN/skip_s300.zip\""
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/My Drive/PLN/skip_s300.zip\n",
            "  inflating: skip_s300.txt           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOkghZf3ES7G"
      },
      "source": [
        "#implementando o modelo skip-gram \n",
        "modelo_skipgram = KeyedVectors.load_word2vec_format(\"skip_s300.txt\")"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2656vx_GOEY"
      },
      "source": [
        "# função para somar os vetores\n",
        "import numpy as np\n",
        "\n",
        "def somaVetores(tokens, modelo):\n",
        "  vetorSomado = np.zeros(300)\n",
        "  for token in tokens:\n",
        "    try:\n",
        "      vetorSomado += modelo.get_vector(token)\n",
        "    except:\n",
        "      if token.isnumeric():\n",
        "        token = \"0\"*len(token)\n",
        "        vetorSomado += modelo.get_vector(token)\n",
        "      else:\n",
        "        vetorSomado += modelo.get_vector(\"unknown\")\n",
        "\n",
        "  return vetorSomado"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YbfgtuUGG9n"
      },
      "source": [
        "#função para tratar os dados e tokenizar o corpus\n",
        "def vetoresTextos(textos, modelo):\n",
        "  x = len(textos)\n",
        "  y = 300\n",
        "  vetores = np.zeros((x,y))\n",
        "\n",
        "  for itexto in range(x):\n",
        "    tokens = tratarTokenizar(textos.iloc[itexto])\n",
        "    vetores[itexto] = somaVetores(tokens, modelo)\n",
        "\n",
        "  return vetores"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwCZETFkElOG"
      },
      "source": [
        "#gerando vetores de treino e vetores de teste com skip-gram\n",
        "vetores_treino_skipgram = vetoresTextos(criticas_treino.texto, modelo_skipgram)\n",
        "vetores_teste_skipgram = vetoresTextos(criticas_teste.texto, modelo_skipgram)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8l4r-LhZGpMB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df63eecf-fadc-49a6-ca89-e63f571ac13e"
      },
      "source": [
        "#observando as dimensões dos vetores\n",
        "print(vetores_treino_skipgram.shape)\n",
        "print(vetores_teste_skipgram.shape)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(39567, 300)\n",
            "(9892, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLtFU7uWGYQf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c53b5c-d3ea-44a3-ae3c-64a035126029"
      },
      "source": [
        "#implementando a classificação com regressão logística e o método skip-gram\n",
        "classificador_skipgram = LogisticRegression(max_iter=200)\n",
        "classificador_skipgram.fit(vetores_treino_skipgram, criticas_treino.sentimento)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=200,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3I1rZGMG-OX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bc8d3ea-feda-47c3-9bd2-08e9d0f96a86"
      },
      "source": [
        "categorias_previstas_skipgram = classificador_skipgram.predict(vetores_teste_skipgram)\n",
        "relatorio_skipgram = classification_report(criticas_teste.sentimento, categorias_previstas_skipgram)\n",
        "print(relatorio_skipgram)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negativo       0.81      0.81      0.81      4991\n",
            "    positivo       0.81      0.81      0.81      4901\n",
            "\n",
            "    accuracy                           0.81      9892\n",
            "   macro avg       0.81      0.81      0.81      9892\n",
            "weighted avg       0.81      0.81      0.81      9892\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neabTYbGHG5-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d249256-11d2-4027-ae17-d7af584554dd"
      },
      "source": [
        "print(relatorio)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negativo       0.81      0.81      0.81      4991\n",
            "    positivo       0.80      0.80      0.80      4901\n",
            "\n",
            "    accuracy                           0.80      9892\n",
            "   macro avg       0.80      0.80      0.80      9892\n",
            "weighted avg       0.80      0.80      0.80      9892\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSsxZkprHksQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "f2051311-7f5f-43ec-8056-394a1eb9bb8d"
      },
      "source": [
        "criticas_treino.head()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>texto</th>\n",
              "      <th>sentimento</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>19425</th>\n",
              "      <td>Tornado humano de 1976 é, em muitos aspectos, ...</td>\n",
              "      <td>positivo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22621</th>\n",
              "      <td>Efeitos surpreendentes para um filme deste tem...</td>\n",
              "      <td>positivo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42506</th>\n",
              "      <td>Este é um filme japonês, mas há um pouco de In...</td>\n",
              "      <td>negativo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6595</th>\n",
              "      <td>Eu não sou muito para filmes de \"policial\", ma...</td>\n",
              "      <td>positivo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41850</th>\n",
              "      <td>... Ruim em ser intencionalmente ruim ... Esta...</td>\n",
              "      <td>negativo</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   texto sentimento\n",
              "19425  Tornado humano de 1976 é, em muitos aspectos, ...   positivo\n",
              "22621  Efeitos surpreendentes para um filme deste tem...   positivo\n",
              "42506  Este é um filme japonês, mas há um pouco de In...   negativo\n",
              "6595   Eu não sou muito para filmes de \"policial\", ma...   positivo\n",
              "41850  ... Ruim em ser intencionalmente ruim ... Esta...   negativo"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti32lmMG-jZp"
      },
      "source": [
        "# Podemos observar que a classificação com o  método skip-gram apresentou uma acurácia média 1% maior que o método CBOW."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5Y0x-vV-2o4"
      },
      "source": [
        "# Comparando o word2vec que chegou a acurácia média de 81% e o Bag of Words utilizado nas unidades 1 e 2 que chegou a 79% de acurácia.Podemos destacar o menor tempo de execução no word2vec e menor tempo para tratamento e tokenização dos dados. A diferença na acurácia foi menor que 2%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q98Cd4Uf_xlj"
      },
      "source": [
        "#Vale aqui destacar que utilizando o vetorzador TF-IDF e bigramas, na unidade 2 chegamos a uma acurácia 88%. Acurácia maior em 7 pontos percentuais para este corpus de textos, do que obtida no word2vec. Cabe para estudos futuros implementar um melhor tratamento e vetorização do , a fim de obter uma acurácia profissioanl, que possa ser utilizada para treinamento de agentes como chatbots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMwq_rbhA1cq"
      },
      "source": [
        "#REFERÊNCIAS:\n",
        "https://medium.com/@everton.tomalok/word2vec-e-sua-importância-na-etapa-de-pré-processamento-d0813acfc8ab\n",
        "https://medium.com/@zafaralibagh6/a-simple-word2vec-tutorial-61e64e38a6a1\n",
        "\n",
        "https://towardsdatascience.com/understanding-word2vec-embedding-in-practice-3e9b8985953"
      ]
    }
  ]
}